title: Text generation Webui
name: textgen
use_python: true
python_version: 10

prepare_repo: |-
  # Remove stale symlink to avoid pull conflicts
  rm -rf $LINK_MODEL_TO

  TARGET_REPO_DIR=$REPO_DIR \
  TARGET_REPO_BRANCH="main" \
  TARGET_REPO_URL="https://github.com/oobabooga/text-generation-webui" \
  UPDATE_REPO=${{ name|upper }}_UPDATE_REPO \
  UPDATE_REPO_COMMIT=${{ name|upper }}_UPDATE_REPO_COMMIT \
  bash $current_dir/../utils/prepare_repo.sh

prepare_env: |-
  cd $REPO_DIR
  pip install torch torchvision torchaudio
  pip install -r requirements.txt

  mkdir -p repositories
  cd repositories
  TARGET_REPO_DIR=$REPO_DIR/repositories/GPTQ-for-LLaMa \
  TARGET_REPO_BRANCH="cuda" \
  TARGET_REPO_URL="https://github.com/qwopqwop200/GPTQ-for-LLaMa.git" \
  bash $current_dir/../utils/prepare_repo.sh
  
  cd GPTQ-for-LLaMa
  python setup_cuda.py install

  pip uninstall -y llama-cpp-python
  CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir

  pip install deepspeed

  pip install xformers

download_model: |-
  # Prepare model dir and link it under the models folder inside the repo
  mkdir -p $MODEL_DIR
  rm -rf $LINK_MODEL_TO
  ln -s $MODEL_DIR $LINK_MODEL_TO
  if [[ ! -f $MODEL_DIR/config.yaml ]]; then 
      current_dir_save=$(pwd) 
      cd $REPO_DIR
      commit=$(git rev-parse HEAD)
      wget -q https://raw.githubusercontent.com/oobabooga/text-generation-webui/$commit/models/config.yaml -P $MODEL_DIR
      cd $current_dir_save
  fi


  bash $current_dir/../utils/llm_model_download.sh

action_before_start: ""

start: |-
  cd $REPO_DIR
  if [ -v {{ name|upper }}_ENABLE_OPENAI_API ] && [ ! -z "${{ name|upper }}_ENABLE_OPENAI_API" ];then
    loader_arg=""
    if echo "${{ name|upper }}_OPENAI_MODEL" | grep -q "GPTQ"; then
      loader_arg="--loader exllama"
    fi
    if echo "${{ name|upper }}_OPENAI_MODEL" | grep -q "LongChat"; then
      loader_arg+=" --max_seq_len 8192 --compress_pos_emb 4"
    fi
    PYTHONUNBUFFERED=1 OPENEDAI_PORT=7013 nohup python server.py --listen-port ${{ name|upper }}_PORT --xformers --model ${{ name|upper }}_OPENAI_MODEL $loader_arg --extensions openai ${EXTRA_{{ name|upper }}_ARGS} > $LOG_DIR/{{ name }}.log 2>&1 &
  else
    PYTHONUNBUFFERED=1 nohup python server.py  --listen-port ${{ name|upper }}_PORT --xformers --chat ${EXTRA_{{ name|upper }}_ARGS} > $LOG_DIR/{{ name }}.log 2>&1 &
  fi
  echo $! > /tmp/{{ name }}.pid

custom_start: ""
custom_reload: ""
custom_stop: ""

export_required_env: ""
other_commands: |-
  export MODEL_DIR=${{ '{' ~ name|upper }}_MODEL_DIR:-"$DATA_DIR/llm-models"}
  export REPO_DIR=${{ '{' ~ name|upper }}_REPO_DIR:-"$OUTPUTS_DIR/text-generation-webui"}

  export {{ name|upper }}_PORT=${{ '{' ~ name|upper }}_PORT:-7009}
  export {{ name|upper }}_OPENAI_API_PORT=${{ '{' ~ name|upper }}_OPENAI_API_PORT:-7013}
  export EXPOSE_PORTS="$EXPOSE_PORTS:${{ name|upper }}_PORT:{{ name|upper }}_OPENAI_API_PORT"
  export PORT_MAPPING="$PORT_MAPPING:{{ name }}:{{ name }}_openai_api"
  export HUGGINGFACE_TOKEN=$HF_TOKEN

  export LINK_MODEL_TO=${{ '{' ~ name|upper }}_LINK_MODEL_TO:-"$REPO_DIR/models"}